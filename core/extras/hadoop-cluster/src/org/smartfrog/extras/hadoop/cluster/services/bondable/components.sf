/** (C) Copyright 2009 Hewlett-Packard Development Company, LP

 This library is free software; you can redistribute it and/or
 modify it under the terms of the GNU Lesser General Public
 License as published by the Free Software Foundation; either
 version 2.1 of the License, or (at your option) any later version.

 This library is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 Lesser General Public License for more details.

 You should have received a copy of the GNU Lesser General Public
 License along with this library; if not, write to the Free Software
 Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

 For more information: www.smartfrog.org

 */

#include "/org/smartfrog/extras/hadoop/cluster/components.sf"

/*

This is where a dynamic cluster set is put together.

Requirements
- bring up a Hadoop cluster in a set of machines
- take predefined names for fixing up the fs URL
- deploy over RMI
- deploy with SCP/SSH to remote VMs
- deploy via the RESTy long-haul APIs (with username, passwd)
- have helper component to deploy locally to do the bulk deploy to >1 machine over SSH

Assumptions
- the cluster is short lived; temp dirs are set up for the life of the system
- the local machine is potentially remote from the target
- the master hosts NN and JT in separate processes
- each worker hosts a TT with binding.tasktracker.slots  of TT slots
- each worker hosts a Datanode
- the TT comes up in a child JVM, as it may run some jobs in-VM, with consequences
- the Datanode comes up in its own JVM
- the workers all need to know the hostname of the master
- port numbers for NN and JT are fixed
- the remote machines are running SF
- all remote machines are up to date with the SF RPMS, including sf-hadoop
- no need to make the config persistent

For RMI deploy
- Hadoop doesn't work with proper security, so you can't do a secure SF cluster. You should only use
  RMI within a private cluster

For SCP/SSH deploy
- there is no requirement for the individual machines to talk to each other
- the remote machines are running SSHD
- the remote machines support deploy-by-copy in their default.sf; this lets us push out new deployments
  with SCP only, no need for remote commands.
- need an SF workflow component to parse a resource with some properties, save it to a file

For longhaul deploy (when implemented)
- sf-longhaul installed and running

Implementation details
- We declare a separate cluster definition that is shared everywhere, Binding
- This is deployed on every host
- It picks up config from client-side properties, which must be set when parsing. This is how
  Mombasa parses the description with the JVM properties

Demo plan
- ant target to deploy a master to a target host driven by properties, defaults to localhost
- ant target to deploy a worker, requiring the name of the master host
- ant target to expand the file, then use the ssh ant task


Test plan
- deploy a master and worker to same box, wait for cluster to go live
-

*/


/**
  * These are JVM properties to set before you parse a bondable configuration, it
  * will let you set the bindings to point to the machines chosen.
  * Failure to set the bindings is an error.
  */




/**
 by default this will deploy nothing; you
 have to opt for either a master or a worker
 */



BindableCluster extends FullCluster {

  master false;
  worker false;
  /**
   * This is the hostname of the master, which hosts both the NN and
   */
  binding.master.hostname PROPERTY binding.master.hostname;

  /* to allow for future tweaking, this is split up */
  binding.namenode.hostname binding.master.hostname;
  binding.jobtracker.hostname binding.master.hostname;

  /**
   * The number of slots for a tasktracker
   */
  binding.tasktracker.slots IPROPERTY binding.tasktracker.slots;


  //fix the namenode hostname
  namenodeHostname binding.namenode.hostname;
  //fix the JT hostname
  jobtrackerHostname binding.jobtracker.hostname;
  //set the no. of task slots
  max_tasks binding.tasktracker.slots ;

}

/**
 * Deploys nothing, but is a preflight check of all the bindings
 */
CheckMasterWorkerConfig extends BindableCluster {

}


/**
 * the master node
 */
MasterNode extends BindableCluster {
  master true;
  namenode:sfProcessName NAMENODE_PROCESS;
  jobtracker:sfProcessName JOBTRACKER_PROCESS;
}

/**
 * the worker node
 */
WorkerNode extends BindableCluster {
  worker true;
  namenode:sfProcessName NAMENODE_PROCESS;
  tasktracker:sfProcessName TASKTRACKER1_PROCESS;
}

MasterAndWorker extends Compound {
  master MasterNode;
  worker WorkerNode;
}

