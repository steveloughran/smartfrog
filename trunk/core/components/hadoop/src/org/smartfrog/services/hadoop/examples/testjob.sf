/* (C) Copyright 2009 Hewlett-Packard Development Company, LP

This library is free software; you can redistribute it and/or
modify it under the terms of the GNU Lesser General Public
License as published by the Free Software Foundation; either
version 2.1 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public
License along with this library; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

For more information: www.smartfrog.org

*/
#include "/org/smartfrog/services/hadoop/junitmr/healthtests/components.sf"
#include "/org/smartfrog/services/hadoop/examples/smallfile.sf"
#include "/org/smartfrog/services/filesystem/csvfiles/components.sf"

FileSystemLiveSequence extends Sequence {
    description "this is a base sequence that blocks until the filesystem is live"

    namenode TBD;
    datanode TBD;
    datanodeCount 1;
    //the cluster for all the tests
    cluster namenode;

    //this is where we sleep
    startupSleep extends Sleep;
    startupSleep2 extends Sleep;
    
    waitForNameNode extends WaitForNameNodeLive {
      service LAZY PARENT:namenode;
    }

    waitForDataNode extends WaitForDataNodeLive {
      service LAZY PARENT:datanode;
    }

    
}


CopyFileInAndOutSequence extends FileSystemLiveSequence {
  LocalDataDir TBD;

  testDir "/tests/CopyFileInAndOut";
  testDirIn  (testDir ++ "/in");
  testDirOut (testDir ++ "/out");
  testDirWorking (testDir ++ "/working");
  testFile (testDirIn ++ "/in.txt");
  inputFileDFS testFile;
  outputFileDFS ( testDirOut ++ "/in.txt");
  inputFileLocal (LocalDataDir  ++ "/in.txt")
  outputFileLocal (LocalDataDir  ++ "/out.txt")

  inputFile extends HealthTestsTextFile {
    filename inputFileLocal;
  };


  CopyDataIn extends DfsCopyFileInWorkflow {
    cluster LAZY PARENT:cluster;
    source inputFileLocal;
    dest testFile;
  }

  SourceExists extends DfsFileExistsWorkflow {
    cluster LAZY PARENT:cluster;
    path inputFileDFS;
    minSize 40;
  }

  lsIn extends DfsListDir {
    cluster LAZY PARENT:cluster;
    path  testDirIn;
  }

  ResultExists extends DfsFileExistsWorkflow {
    cluster LAZY PARENT:cluster;
    path testFile;
  }

  CopyDataOut extends DfsCopyFileOutWorkflow {
    cluster LAZY PARENT:cluster;
    source testFile;
    dest outputFileLocal;
  }


  fs.default.name.Checker extends ConfCheckerWorkflow {
    cluster LAZY PARENT:cluster;
    fs.default.name HadoopConfiguration:FS_FILE_URL;
    conf.matches [
      ["fs.default.name",HadoopConfiguration:FS_FILE_URL]
    ];
  }


  /**
   * need to extend ls to allow overrides
   */
   lsLocalData extends DfsListDir {
    cluster LAZY PARENT:cluster;
    fs.default.name HadoopConfiguration:FS_FILE_URL;
    path  LocalDataDir;
    minTotalFileSize 40;
    minFileCount 1;
  }

  checkDataWasCopied extends FileExistsWorkflow {
    filename outputFileLocal;
  }
}



/**
 * A sequence of operations on a cluster */
FileSystemTestSequence1 extends FileSystemLiveSequence {



    rootDirExists extends DfsPathExistsWorkflow {
      cluster LAZY PARENT:cluster;
      path "/";
    }

    touch extends DfsCreateFile {
      cluster LAZY PARENT:cluster;
      path "/test-filename";
      text "this is a very small file for Hadoop";
    }

    assertFileExists extends DfsPathExistsWorkflow {
      cluster LAZY PARENT:cluster;
      path touch:path;
    }

  }

  FileCreation extends Compound  {
    cluster TBD;
    dest TBD;

    sfShouldTerminate true;

    sourceFile extends InlineTupleSource {
      data [
        ["one",43],
        ["two", "43"],
        ["one",12]
      ]

    }

    UploadHadoopCsvFile extends TuplesToHadoop {
      cluster LAZY PARENT:cluster;
      source LAZY sourceFile;
      dest testFile;

    }
  }


/**
 * A sequence of operations on a cluster */
FileSystemTestSequence extends FileSystemLiveSequence {

    rootDirExists extends DfsDirExistsWorkflow {
      cluster LAZY PARENT:cluster;
      path "/";
      verbose true;
    }

    touch extends DfsCreateFileWorkflow {
      cluster LAZY PARENT:cluster;
      path "/test-filename";
      text "0123456789012345678901234567890123456789";
    }

    assertFileExists extends DfsFileExistsWorkflow {
      cluster LAZY PARENT:cluster;
      path touch:path;
      minSize 40;
      verbose true;
    }

    /** this used to check for exactly 1 file, but as some workers create a tmp dir, this doesnt work
     all the time */
    rootDirHasAtLeastOneEntry extends rootDirExists {
      minFileCount 1;
      minTotalFileSize assertFileExists:minSize;
    }

  }

  FileCreation extends Compound  {
    cluster TBD;
    dest TBD;

    sfShouldTerminate true;

    sourceFile extends InlineTupleSource {
      data [
        ["one",43],
        ["two", "43"],
        ["one",12]
      ]

    }

    UploadHadoopCsvFile extends TuplesToHadoop {
      cluster LAZY PARENT:cluster;
      source LAZY sourceFile;
      dest testFile;
    }
  }


/**
 Component that sleeps for a limited period
 */
Sleep extends Delay {
  time STARTUP_SLEEP_TIME;
}

/**
 * sequence that waits for the filesystem and job trackers
 */

JobTrackerLiveSequence extends FileSystemLiveSequence {

    
  jobtracker TBD;

  //add the check for the job tracker
  requireJobTrackerLive true;
 
  //wait for the filesystem to come up, and check for the job tracker
  waitForJobTracker extends WaitForFilesystemLive {
    service LAZY PARENT:jobtracker;
    jobTrackerLive PARENT:requireJobTrackerLive;
  } 

}


/**
 * this is a complete MR workflow
 */
JobTrackerSourceFileSetupSequence extends JobTrackerLiveSequence {
  LocalDataDir PROPERTY test.work.dir;

  fs.default.name cluster:fs.default.name;
  testURL (fs.default.name ++ testDir);
  testDir "/tests/mrtestsequence";
  testDirIn  (testDir ++ "/in");
  testDirOut (testDir ++ "/out");
  testDirWorking (testDir ++ "/working");
  testFile (testDirIn ++ "/in.txt");
  inputFileDFS testFile;
  partName "/part-00000";
  outputFileDFS ( testDirOut ++ partName );
  inputFileLocal (LocalDataDir  ++ "/in.txt");
  outputDirLocal (LocalDataDir  ++ "/out");
  outputFileLocal (outputDirLocal  ++ partName );

  mapred.job.tracker LAZY jobtracker:live.mapred.job.tracker;
  hadoop.tmp.dir "/tmp";
  hadoop.tmp.URL (fs.default.name ++ hadoop.tmp.dir);
  jobTimeout JOB_EXECUTE_TIMEOUT;

  //mapred.system.dir "/tmp/hadoop/mapred/system";

  //mapred.child.java.opts "-Xmx512m -d64 -server";
  mapred.tasktracker.map.tasks.maximum 5;
  mapred.tasktracker.reduce.tasks.maximum 1;

  io.sort.record.percent 0.1F;
  mapred.input.dir (fs.default.name ++ testDirIn);
  mapred.output.dir (fs.default.name ++ testDirOut);
  mapred.working.dir (fs.default.name ++ testDirWorking);

  inputFile extends HealthTestsTextFile {
    filename inputFileLocal;
  }

  CopyDataIn extends DfsCopyFileInWorkflow {
    cluster LAZY PARENT:cluster;
    source inputFileLocal;
    dest testFile;
  }

  SourceExists extends DfsFileExistsWorkflow {
    cluster LAZY PARENT:cluster;
    path inputFileDFS;
    minSize 40;
  }

  lsIn extends DfsListDir {
    cluster LAZY PARENT:cluster;
    path  testDirIn;
  }

  mkOutputDir extends DfsCreateDir {
    cluster LAZY PARENT:cluster;
    path  testDirOut;
  }

}


TestJob extends BlockingJobSubmitter {
  name TBD;
  cluster TBD;
  jobtracker TBD;
  fs.default.name cluster:fs.default.name;
  //we get the live URL to the job tracker
  mapred.job.tracker LAZY jobtracker:live.mapred.job.tracker;
  hadoop.tmp.dir "/tmp";

  //mapred.system.dir "/tmp/hadoop/mapred/system";

  mapred.child.java.opts "-Xmx512m";
  mapred.tasktracker.map.tasks.maximum 5;
  mapred.tasktracker.reduce.tasks.maximum 1;
  mapred.working.dir (hadoop.tmp.dir ++ "/" ++ name ++ "/working");
  mapred.local.dir (hadoop.tmp.dir ++ "/" ++ name ++ "local");
  keep.failed.task.files true;

  mapred.map.max.attempts 1;
  mapred.reduce.max.attempts 1;
}

TestJobNoFile extends TestJob {
  filename "";
  fileRequired false;
  mapred.jar NULL;
}


MapReduceSequence extends JobTrackerSourceFileSetupSequence {

  job extends TestJobNoFile {
    cluster LAZY PARENT:cluster;
    jobtracker LAZY PARENT:jobtracker;
    results LAZY PARENT;
    name "testsubmission";
    fs.default.name PARENT:fs.default.name;
    mapred.input.dir PARENT:mapred.input.dir;
    mapred.output.dir PARENT:mapred.output.dir;
    //mapred.local.dir PARENT:mapred.local.dir;
    //mapred.job.split.file (testDir ++ "/split");
  //we get the live URL to the job tracker
    mapred.job.tracker LAZY jobtracker:live.mapred.job.tracker;
    mapred.working.dir (hadoop.tmp.URL ++ "/" ++ name ++ "/working");
    mapred.local.dir (hadoop.tmp.URL ++ "/" ++ name ++ "local");
  }

  lsOut extends DfsListDir {
    cluster LAZY PARENT:cluster;
    path  mapred.output.dir;
    minTotalFileSize 40;
    minFileCount 1;
  }

  OutDirHasOneFile extends DfsDirExistsWorkflow {
    cluster LAZY PARENT:cluster;
    path mapred.output.dir;
    minFileCount 1;
    verbose true;
  }

 //problem here is the pathname
  ResultExists extends DfsFileExistsWorkflow {
    cluster LAZY PARENT:cluster;
    path outputFileDFS;
    minSize 40;
  }

  CopyDataOut extends DfsCopyFilesWorkflow {
    cluster LAZY PARENT:cluster;
    source mapred.output.dir;
    dest outputDirLocal;

    //source filesystem URL
    sourceFS PARENT:fs.default.name;
    //URL to the destination filesystem
    destFS HadoopConfiguration:FS_FILE_URL;

    //any files that match the pattern
    pattern "part-.+";
    minFileCount 1;
    maxFileCount 1;
  }


  fs.default.name.Checker extends ConfCheckerWorkflow {
    cluster LAZY PARENT:cluster;
    fs.default.name HadoopConfiguration:FS_FILE_URL;
    conf.matches [
      ["fs.default.name",HadoopConfiguration:FS_FILE_URL]
    ];
  }


  /**
   * need to extend ls to allow overrides
   */
   lsLocalData extends DfsListDir {
    cluster LAZY PARENT:cluster;
    fs.default.name HadoopConfiguration:FS_FILE_URL;
    path  outputDirLocal;
    minTotalFileSize 40;
    minFileCount 1;
  }

  checkDataWasCopied extends FileExistsWorkflow {
    filename outputFileLocal;
  }


}